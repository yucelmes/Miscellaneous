{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "#\n",
    "# Copyright (c) 2016 Matthew Earl\n",
    "# \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "# \n",
    "#     The above copyright notice and this permission notice shall be included\n",
    "#     in all copies or substantial portions of the Software.\n",
    "# \n",
    "#     THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n",
    "#     OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n",
    "#     MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN\n",
    "#     NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n",
    "#     DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\n",
    "#     OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE\n",
    "#     USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Routines for training the network.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "__all__ = (\n",
    "    'train',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import glob\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "import common\n",
    "import gen\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def code_to_vec(p, code):\n",
    "    def char_to_vec(c):\n",
    "        y = numpy.zeros((len(common.CHARS),))\n",
    "        y[common.CHARS.index(c)] = 1.0\n",
    "        return y\n",
    "\n",
    "    c = numpy.vstack([char_to_vec(c) for c in code])\n",
    "\n",
    "    return numpy.concatenate([[1. if p else 0], c.flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(img_glob):\n",
    "    for fname in sorted(glob.glob(img_glob)):\n",
    "        im = cv2.imread(fname)[:, :, 0].astype(numpy.float32) / 255.\n",
    "        code = fname.split(\"\\\\\")[1][9:16]\n",
    "        p = fname.split(\"\\\\\")[1][17] == '1'\n",
    "        yield im, code_to_vec(p, code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unzip(b):\n",
    "    xs, ys = zip(*b)\n",
    "    xs = numpy.array(xs)\n",
    "    ys = numpy.array(ys)\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch(it, batch_size):\n",
    "    out = []\n",
    "    for x in it:\n",
    "        out.append(x)\n",
    "        if len(out) == batch_size:\n",
    "            yield out\n",
    "            out = []\n",
    "    if out:\n",
    "        yield out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _main(q, args, kwargs):\n",
    "    try:\n",
    "        for item in f(*args, **kwargs):\n",
    "            q.put(item)\n",
    "    finally:\n",
    "        q.close()\n",
    "\n",
    "\n",
    "def mpgen(f):\n",
    "\n",
    "    @functools.wraps(f)\n",
    "    def wrapped(*args, **kwargs):\n",
    "        q = multiprocessing.Queue(3) \n",
    "        proc = multiprocessing.Process(target=_main,\n",
    "                                       args=(q, args, kwargs))\n",
    "        proc.start()\n",
    "        try:\n",
    "            while True:\n",
    "                item = q.get()\n",
    "                yield item\n",
    "        finally:\n",
    "            proc.terminate()\n",
    "            proc.join()\n",
    "\n",
    "    return wrapped\n",
    "\n",
    "\n",
    "@mpgen\n",
    "def read_batches(batch_size):\n",
    "    g = gen.generate_ims()\n",
    "    def gen_vecs():\n",
    "        for im, c, p in itertools.islice(g, batch_size):\n",
    "            yield im, code_to_vec(p, c)\n",
    "\n",
    "    while True:\n",
    "        yield unzip(gen_vecs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss(y, y_):\n",
    "    # Calculate the loss from digits being incorrect.  Don't count loss from\n",
    "    # digits that are in non-present plates.\n",
    "    digits_loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                                          tf.reshape(y[:, 1:],\n",
    "                                                     [-1, len(common.CHARS)]),\n",
    "                                          tf.reshape(y_[:, 1:],\n",
    "                                                     [-1, len(common.CHARS)]))\n",
    "    digits_loss = tf.reshape(digits_loss, [-1, 7])\n",
    "    digits_loss = tf.reduce_sum(digits_loss, 1)\n",
    "    digits_loss *= (y_[:, 0] != 0)\n",
    "    digits_loss = tf.reduce_sum(digits_loss)\n",
    "\n",
    "    # Calculate the loss from presence indicator being wrong.\n",
    "    presence_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                                                          y[:, :1], y_[:, :1])\n",
    "    presence_loss = 7 * tf.reduce_sum(presence_loss)\n",
    "\n",
    "    return digits_loss, presence_loss, digits_loss + presence_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(learn_rate, report_steps, batch_size, initial_weights=None):\n",
    "    \"\"\"\n",
    "    Train the network.\n",
    "\n",
    "    The function operates interactively: Progress is reported on stdout, and\n",
    "    training ceases upon `KeyboardInterrupt` at which point the learned weights\n",
    "    are saved to `weights.npz`, and also returned.\n",
    "\n",
    "    :param learn_rate:\n",
    "        Learning rate to use.\n",
    "\n",
    "    :param report_steps:\n",
    "        Every `report_steps` batches a progress report is printed.\n",
    "\n",
    "    :param batch_size:\n",
    "        The size of the batches used for training.\n",
    "\n",
    "    :param initial_weights:\n",
    "        (Optional.) Weights to initialize the network with.\n",
    "\n",
    "    :return:\n",
    "        The learned network weights.\n",
    "\n",
    "    \"\"\"\n",
    "    x, y, params = model.get_training_model()\n",
    "\n",
    "    y_ = tf.placeholder(tf.float32, [None, 7 * len(common.CHARS) + 1])\n",
    "\n",
    "    digits_loss, presence_loss, loss = get_loss(y, y_)\n",
    "    train_step = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n",
    "\n",
    "    best = tf.argmax(tf.reshape(y[:, 1:], [-1, 7, len(common.CHARS)]), 2)\n",
    "    correct = tf.argmax(tf.reshape(y_[:, 1:], [-1, 7, len(common.CHARS)]), 2)\n",
    "\n",
    "    if initial_weights is not None:\n",
    "        assert len(params) == len(initial_weights)\n",
    "        assign_ops = [w.assign(v) for w, v in zip(params, initial_weights)]\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    def vec_to_plate(v):\n",
    "        return \"\".join(common.CHARS[i] for i in v)\n",
    "\n",
    "    def do_report():\n",
    "        r = sess.run([best,\n",
    "                      correct,\n",
    "                      tf.greater(y[:, 0], 0),\n",
    "                      y_[:, 0],\n",
    "                      digits_loss,\n",
    "                      presence_loss,\n",
    "                      loss],\n",
    "                     feed_dict={x: test_xs, y_: test_ys})\n",
    "        num_correct = numpy.sum(\n",
    "                        numpy.logical_or(\n",
    "                            numpy.all(r[0] == r[1], axis=1),\n",
    "                            numpy.logical_and(r[2] < 0.5,\n",
    "                                              r[3] < 0.5)))\n",
    "        r_short = (r[0][:190], r[1][:190], r[2][:190], r[3][:190])\n",
    "        for b, c, pb, pc in zip(*r_short):\n",
    "            print(\"{} {} <-> {} {}\".format(vec_to_plate(c), pc,\n",
    "                                           vec_to_plate(b), float(pb)))\n",
    "        num_p_correct = numpy.sum(r[2] == r[3])\n",
    "\n",
    "        print (\"B{:3d} {:2.02f}% {:02.02f}% loss: {} \"\n",
    "               \"(digits: {}, presence: {}) |{}|\").format(\n",
    "            batch_idx,\n",
    "            100. * num_correct / (len(r[0])),\n",
    "            100. * num_p_correct / len(r[2]),\n",
    "            r[6],\n",
    "            r[4],\n",
    "            r[5],\n",
    "            \"\".join(\"X \"[numpy.array_equal(b, c) or (not pb and not pc)]\n",
    "                                           for b, c, pb, pc in zip(*r_short)))\n",
    "\n",
    "    def do_batch():\n",
    "        sess.run(train_step,\n",
    "                 feed_dict={x: batch_xs, y_: batch_ys})\n",
    "        if batch_idx % report_steps == 0:\n",
    "            do_report()\n",
    "\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.95)\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "        sess.run(init)\n",
    "        if initial_weights is not None:\n",
    "            sess.run(assign_ops)\n",
    "\n",
    "        test_xs, test_ys = unzip(list(read_data(\"test/*.png\"))[:50])\n",
    "\n",
    "        try:\n",
    "            last_batch_idx = 0\n",
    "            last_batch_time = time.time()\n",
    "            batch_iter = enumerate(read_batches(batch_size))\n",
    "            for batch_idx, (batch_xs, batch_ys) in batch_iter:\n",
    "                do_batch()\n",
    "                if batch_idx % report_steps == 0:\n",
    "                    batch_time = time.time()\n",
    "                    if last_batch_idx != batch_idx:\n",
    "                        print(\"time for 60 batches {}\".format(\n",
    "                            60 * (last_batch_time - batch_time) /\n",
    "                                            (last_batch_idx - batch_idx)))\n",
    "                        last_batch_idx = batch_idx\n",
    "                        last_batch_time = batch_time\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            last_weights = [p.eval() for p in params]\n",
    "            numpy.savez(\"weights.npz\", *last_weights)\n",
    "            return last_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #if len(sys.argv) > 1:\n",
    "    #    f = numpy.load(sys.argv[1])\n",
    "    #    initial_weights = [f[n] for n in sorted(f.files,\n",
    "    #                                            key=lambda s: int(s[4:]))]\n",
    "    #else:\n",
    "    initial_weights = None\n",
    "\n",
    "    train(learn_rate=0.001,\n",
    "          report_steps=20,\n",
    "          batch_size=50,\n",
    "          initial_weights=initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
